{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import Bio   \n",
    "\n",
    "from Bio.PDB import *\n",
    "from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n",
    "import ipywidgets\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from jproperties import Properties\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModel, TrainingArguments, Trainer\n",
    "from motif_utils import seq2kmer # Soruced from https://github.com/jerryji1993/DNABERT\n",
    "import torch\n",
    "#from datasets import Dataset\n",
    "#import evaluate\n",
    "import json\n",
    "#from load_data import create_dataset, explode_dna\n",
    "import wandb\n",
    "\n",
    "from transformers.onnx import FeaturesManager\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import natsort\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensure all images are the right dimensions for ResNet\n",
    "'''\n",
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.ToTensor()])\n",
    "#dataset = datasets.ImageFolder('./data/MouseImages/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all data we need\n",
    "'''\n",
    "with open(\"./data/gene_symbol_protein_sequences.pkl\", 'rb') as file:\n",
    "        protein =  pd.read_pickle(file)\n",
    "with open(\"./data/capstone_body_weight_Statistical_effect_size_analysis_genotype_early_adult_scaled_13022023_gene_symbol_harmonized.pkl\", 'rb') as file:\n",
    "        effect =  pd.read_pickle(file)\n",
    "\n",
    "def filter_data(effect, protein):\n",
    "    '''\n",
    "    Merge data and reduce dataframe to only nessisary values\n",
    "    '''\n",
    "    effect = effect[['gene_symbol_harmonized','est_f_ea','p_f_ea']].copy()\n",
    "    final = pd.merge(effect, protein, left_on=\"gene_symbol_harmonized\", right_on=\"gene_symbol_harmonized\", how =\"left\")\n",
    "    return final [[\"gene_symbol_harmonized\", \"UniqueIdentifier\", \"est_f_ea\"]]\n",
    "df = filter_data(effect, protein)\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5313\n",
      "5313\n"
     ]
    }
   ],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    '''\n",
    "    This creates a custom dataset class for pytorch so that we can use the pytorch API for our dataset.\n",
    "    Features: \n",
    "    X -> Tensor image\n",
    "    Y -> Weight Change\n",
    "    Name -> Protien name\n",
    "    '''\n",
    "    def __init__(self, main_dir, transform, effect_df):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.x =[]\n",
    "        self.y =[]\n",
    "        self.name=[]\n",
    "        for img in all_imgs:\n",
    "            accenssion = img.split(\"-\")[1]\n",
    "            if accenssion in list(effect_df.UniqueIdentifier):\n",
    "                self.x.append(os.path.join(self.main_dir, img))\n",
    "                self.name.append(accenssion)\n",
    "                self.y.append(effect_df[effect_df.UniqueIdentifier == accenssion].est_f_ea.values[0])\n",
    "        print(len(self.x))\n",
    "        print(len(self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __mean__(self):\n",
    "        return np.mean(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.x[idx]).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return {\"x\":tensor_image, \"y\": torch.tensor(self.y[idx])}\n",
    "image_dataset = CustomDataSet(r'./data/MouseImages/', transform=transform, effect_df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    '''\n",
    "    This modifies the original resnet model such that it ends with a linear layer of size 1, \n",
    "    and then pass through a tanh to limit the output between -1, 1 which is the range in which\n",
    "    we normalized the data too. So it can only predict valid estimates\n",
    "    '''\n",
    "    def __init__(self, base_model=\"resnet18\", pretrained=False, frozen=False):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', base_model, pretrained=pretrained)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        if frozen:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.model.fc = nn.Linear(num_ftrs,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.model(x))\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class epochMetrics():\n",
    "  '''\n",
    "  This class keeps track of the relavent metrics for our training\n",
    "  '''\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.avg = 0\n",
    "\n",
    "    self.l1_sum = 0\n",
    "    self.l1_avg = 0\n",
    "\n",
    "    self.l2_sum = 0\n",
    "    self.l2_avg = 0\n",
    "\n",
    "    self.r2_sum = 0\n",
    "    self.r2_avg = 0\n",
    "    self.count = 0\n",
    "    \n",
    "\n",
    "  def update(self, outputs, y):\n",
    "    with torch.no_grad():\n",
    "      l1 = nn.L1Loss().to(\"cuda\")\n",
    "      l2 = nn.MSELoss().to(\"cuda\")\n",
    "\n",
    "      self.count+=1\n",
    "      self.l1_sum += l1(outputs, y).item()\n",
    "      self.l1_avg = self.l1_sum/self.count\n",
    "\n",
    "      self.l2_sum += l2(outputs, y).item()\n",
    "      self.l2_avg = self.l2_sum/self.count\n",
    "\n",
    "\n",
    "  \n",
    "  def log(self):\n",
    "        wandb.log({f\"{self.name} Epoch L1 Loss\": self.l1_avg})\n",
    "        wandb.log({f\"{self.name} Epoch L2 Loss\": self.l2_avg})\n",
    "        wandb.log({f\"{self.name} Epoch RMSE Loss\": np.sqrt(self.l2_avg)})\n",
    "        #wandb.log({f\"{self.name} R2\": self.r2_avg })\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "val_count = 0\n",
    "def train(train_loader, model, criterion, optimizer, config):\n",
    "    '''Standard Training loop for our model '''\n",
    "    model.train()\n",
    "    epoch_metrics = epochMetrics(\"Train\")\n",
    "    predictions = []\n",
    "  \n",
    "    with tqdm(train_loader) as _tqdm:\n",
    "        for batch in _tqdm:\n",
    "            optimizer.zero_grad()\n",
    "            x= batch['x'].to(config[\"device\"])\n",
    "            y = batch['y'].to(config[\"device\"])\n",
    "            outputs = model(x)\n",
    "  \n",
    "            loss = criterion(outputs.squeeze(1), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            #wandb.log({\"Train Actual STD\":torch.std(y)})\n",
    "            #wandb.log({\"Train Predicted STD\":torch.std(outputs.squeeze(1))})\n",
    "            #predictions.append(outputs.squeeze(1).item())\n",
    "            epoch_metrics.update(outputs.squeeze(1), y)\n",
    "    #wandb.log({\"Train Pred STD\":torch.std(predictions)})\n",
    "    #wandb.log({\"Train Pred Mean\":torch.mean(predictions)})\n",
    "    epoch_metrics.log()\n",
    "    \n",
    "    \n",
    "            \n",
    "def val(validate_loader, model, criterion, config):\n",
    "    '''Same as the training loop except in eval mode so it doesn't update the weights of the model'''\n",
    "    model.eval()\n",
    "    epoch_metrics = epochMetrics(\"Val\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(validate_loader) as _tqdm:\n",
    "            for batch in _tqdm:\n",
    "                x= batch['x'].to(config.device)\n",
    "                y = batch['y'].to(config.device)\n",
    "                outputs = model(x)\n",
    "                wandb.log({\"Val Actual STD\":torch.std(y)})\n",
    "                wandb.log({\"Val Predicted STD\":torch.std(outputs.squeeze(1))})\n",
    "                epoch_metrics.update(outputs.squeeze(1), y)\n",
    "\n",
    "    #wandb.log({\"Val Pred STD\":torch.std(predictions)})\n",
    "    #wandb.log({\"Val Pred Mean\":torch.mean(predictions)})\n",
    "    epoch_metrics.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: fmzpw0ii\n",
      "Sweep URL: https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "These are our wandb configs\n",
    "Use config for a single run\n",
    "Use sweep_config for multiple runs in a sweep\n",
    "'''\n",
    "config = {\n",
    "    \"device\":\"cuda\",\n",
    "    \"learning_rate\": .001252,\n",
    "    \"epochs\":12,\n",
    "    \"pretrained\":False,\n",
    "    \"frozen\":False,\n",
    "    \"min_lr\":.0003761,\n",
    "    \"warmup_steps\":93,\n",
    "    \"loss\":\"l1\",\n",
    "    \"epochs\":55,\n",
    "    \"batch_size\":9,\n",
    "    \"base_model\":\"resnet34\",\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    \"parameters\": {\n",
    "        \"device\":{\n",
    "            \"value\":\"cuda\"\n",
    "            },\n",
    "        \"learning_rate\":{\n",
    "            \"max\":1e-2,\n",
    "            \"min\":1e-6\n",
    "            },\n",
    "        \"pretrained\":{\n",
    "            \"value\": True\n",
    "            },\n",
    "        \"frozen\":{\n",
    "            \"value\":False\n",
    "            },\n",
    "        \"min_lr\":{\n",
    "            \"min\":0.0,\n",
    "            \"max\":1e-3\n",
    "            },\n",
    "        \"warmup_steps\":{\n",
    "            \"min\":0,\n",
    "            \"max\":100\n",
    "        },\n",
    "        \"loss\":{\n",
    "            \"value\":\"l1\"\n",
    "            }, #Smooth L1 with beta\n",
    "        \"epochs\":{\n",
    "            \"value\":50,\n",
    "        },\n",
    "        \"batch_size\":{\n",
    "            \"min\":2,\n",
    "            \"max\":16,\n",
    "            },\n",
    "        \"base_model\":{\n",
    "            \"values\":[\"resnet18\",\"resnet34\",\"resnet50\"]\n",
    "            },\n",
    "    },\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    'name': 'Val Epoch L1 Loss',\n",
    "    'goal': 'minimize'   \n",
    "}\n",
    "\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "#wandb.init(config = config, project=\"Protein-Weight\", entity=\"pcoady\")\n",
    "#sweep_id = wandb.sweep(sweep_config, project=\"Protein-Weight\", entity=\"pcoady\")\n",
    "#sweep_id\n",
    "#pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(image_dataset, config = None):\n",
    "  '''\n",
    "  This method takes in the entire dataset, splits it into train/val/test sets\n",
    "  and calculates the baselines for the val set to compare to when running. \n",
    "  '''\n",
    "  train_data,val_data,test_data = torch.utils.data.random_split(image_dataset, [.8, .1, .1 ])\n",
    "\n",
    "  train_y, val_y,test_y = [],[],[]\n",
    "  scaler = MinMaxScaler((-1,1))\n",
    "\n",
    "  for i in tqdm(train_data.indices):\n",
    "    train_y.append(train_data.dataset.y[i])\n",
    "  train_y = torch.tensor(train_y)\n",
    "  scaler.fit(train_y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "  for i in tqdm(train_data.indices):\n",
    "    train_data.dataset.y[i] = scaler.transform(np.array(train_data.dataset.y[i]).reshape(1, -1) ).item()\n",
    "  for i in tqdm(val_data.indices):\n",
    "    val_data.dataset.y[i] = scaler.transform(np.array(val_data.dataset.y[i]).reshape(1, -1) ).item()\n",
    "  #for i in tqdm(test_data.indices):\n",
    "  #  test_data.dataset.y[i] = scaler.transform(np.array(test_data.dataset.y[i]).reshape(1, -1) )\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  for i in tqdm(val_data.indices):\n",
    "    val_y.append(val_data.dataset.y[i])\n",
    "  #for i in tqdm(test_data.indices):\n",
    "  #  test_y.append(test_data.dataset.y[i])\n",
    "\n",
    "  val_y = torch.tensor(val_y)\n",
    "\n",
    "  val_mean = torch.mean(val_y)\n",
    "  val_means = torch.stack([torch.tensor(val_mean) for n in range(len(val_y))])\n",
    "\n",
    "  l1 = nn.L1Loss()\n",
    "  l2 = nn.MSELoss()\n",
    "\n",
    "  baselines = {\"l1_baseline\":l1(val_means, val_y),\n",
    "               \"l2_baseline\": l2(val_means, val_y),\n",
    "               \"RMSE_baseline\": np.sqrt(l2(val_means, val_y)),\n",
    "               \"r2_balseline\": r2_score(val_y, val_means)\n",
    "               }\n",
    "               \n",
    "\n",
    "  train_loader = DataLoader(train_data , batch_size=config[\"batch_size\"], shuffle=True)\n",
    "  val_loader = DataLoader(val_data , batch_size=config[\"batch_size\"], shuffle=True)\n",
    "  #test_loader = DataLoader(test_data , batch_size=3, shuffle=True)\n",
    "\n",
    "  return train_loader, val_loader, baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(config = None):\n",
    "    '''This is the training loop for either a single run or a sweep run in wandb'''\n",
    "    \n",
    "    with wandb.init(project=\"Protein-Weight\", entity=\"pcoady\", config = config):\n",
    "        #gets the wandb config that wandb pushes for the sweep run\n",
    "        config = wandb.config\n",
    "        train_loader, val_loader, baselines = split_data(image_dataset, config)\n",
    "        model = CustomModel(base_model=config.base_model, pretrained=config.pretrained, frozen=config.frozen).to(config.device)\n",
    "        \n",
    "        criterion = nn.L1Loss().to(config.device)\n",
    "        #Watch command will automatically track metrics in our model\n",
    "        wandb.watch(model, criterion=criterion)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        #This controls the warmup steps for our learning rate so it changes throughout time\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=config.warmup_steps, eta_min=config.learning_rate*config.min_lr)\n",
    "        for epoch in range(config.epochs):\n",
    "            #Main training loop, logging baselines at the end to make graphing easier\n",
    "            train(train_loader, model, criterion, optimizer, config)\n",
    "            val(val_loader, model, criterion, config)\n",
    "            scheduler.step()\n",
    "            wandb.log({\"Epoch Step\":epoch})\n",
    "            wandb.log({\"L1 Baseline\": baselines[\"l1_baseline\"]})\n",
    "            wandb.log({\"L2 Baseline\": baselines[\"l2_baseline\"]})\n",
    "            wandb.log({\"RMSE Baseline\": baselines[\"RMSE_baseline\"]})\n",
    "            wandb.log({\"R2 Baseline\": baselines[\"r2_balseline\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(init_config = None):\n",
    "  '''Similar to train_loop but configured for k_fold validation'''\n",
    "  splits=KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "  for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(image_dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    wandb.init(project=\"Protein-Weight\", entity=\"pcoady\", config = init_config)\n",
    "    config = wandb.config\n",
    "    scaler = MinMaxScaler((-1,1))\n",
    "    image_dataset.y = list(scaler.fit_transform(np.array(image_dataset.y).reshape(-1, 1)).reshape(1, -1)[0])\n",
    "    \n",
    "    l1 = nn.L1Loss()\n",
    "    l2 = nn.MSELoss()\n",
    "    y_mean = np.mean(image_dataset.y)\n",
    "    dummpy_regressor = torch.tensor([y_mean for i in range(len(image_dataset.y))])\n",
    "    real_y = torch.tensor(image_dataset.y)\n",
    "    baselines = {\"l1_baseline\":l1(dummpy_regressor, real_y),\n",
    "               \"l2_baseline\": l2(dummpy_regressor, real_y),\n",
    "               \"RMSE_baseline\": np.sqrt(l2(dummpy_regressor, real_y)),\n",
    "               \"r2_balseline\": r2_score(real_y, dummpy_regressor)\n",
    "               }\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(image_dataset, batch_size=config[\"batch_size\"], sampler=train_sampler)\n",
    "    val_loader = DataLoader(image_dataset, batch_size=config[\"batch_size\"], sampler=test_sampler)\n",
    "    \n",
    "    model = CustomModel(base_model=config.base_model, pretrained=config.pretrained, frozen=config.frozen).to(config.device)\n",
    "    criterion = nn.L1Loss().to(config.device)\n",
    "    wandb.watch(model, criterion=criterion)\n",
    "    optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config.warmup_steps, eta_min=config.learning_rate*config.min_lr)\n",
    "    for epoch in range(config.epochs):\n",
    "        train(train_loader, model, criterion, optimizer, config)\n",
    "        val(val_loader, model, criterion, config)\n",
    "        scheduler.step()\n",
    "        wandb.log({\"Epoch Step\":epoch})\n",
    "        wandb.log({\"L1 Baseline\": baselines[\"l1_baseline\"]})\n",
    "        wandb.log({\"L2 Baseline\": baselines[\"l2_baseline\"]})\n",
    "        wandb.log({\"RMSE Baseline\": baselines[\"RMSE_baseline\"]})\n",
    "        wandb.log({\"R2 Baseline\": baselines[\"r2_balseline\"]})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kdo16bgf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: resnet18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdevice: cpu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: l1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_lr: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML\\bert_dna_weight\\wandb\\run-20230319_175915-kdo16bgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pcoady/Protein-Weightt/runs/kdo16bgf' target=\"_blank\">gentle-sweep-1</a></strong> to <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/kdo16bgf' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/kdo16bgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zeusg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "d:\\ML\\bert_dna_weight\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\ML\\bert_dna_weight\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "d:\\ML\\bert_dna_weight\\.venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022471ea56cb44919621db8883fe4524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5e71f8d7294815b8b419255b557b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='7.290 MB of 10.866 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.67095…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch Step</td><td>▁</td></tr><tr><td>Train Actual STD</td><td>▁</td></tr><tr><td>Train Predicted STD</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch Step</td><td>0</td></tr><tr><td>Train Actual STD</td><td>0.52815</td></tr><tr><td>Train Predicted STD</td><td>0.27113</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-1</strong> at: <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/kdo16bgf' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/kdo16bgf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230319_175915-kdo16bgf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run kdo16bgf errored: IndexError('Dimension out of range (expected to be in range of [-1, 0], but got 1)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run kdo16bgf errored: IndexError('Dimension out of range (expected to be in range of [-1, 0], but got 1)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2np7amjf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: resnet34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdevice: cpu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: l1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_lr: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML\\bert_dna_weight\\wandb\\run-20230319_175930-2np7amjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pcoady/Protein-Weightt/runs/2np7amjf' target=\"_blank\">smooth-sweep-2</a></strong> to <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/2np7amjf' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/2np7amjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zeusg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "d:\\ML\\bert_dna_weight\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b318097a1b4944b9b00df389c8c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc4d8427cd341d8bccd05f2df039253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='9.743 MB of 10.862 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.89705…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch Step</td><td>▁</td></tr><tr><td>Train Actual STD</td><td>▁</td></tr><tr><td>Train Predicted STD</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch Step</td><td>0</td></tr><tr><td>Train Actual STD</td><td>0.43764</td></tr><tr><td>Train Predicted STD</td><td>0.43541</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-2</strong> at: <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/2np7amjf' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/2np7amjf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230319_175930-2np7amjf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2np7amjf errored: IndexError('Dimension out of range (expected to be in range of [-1, 0], but got 1)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2np7amjf errored: IndexError('Dimension out of range (expected to be in range of [-1, 0], but got 1)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mfms6xgk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: resnet110\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdevice: cpu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrozen: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: l1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_lr: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML\\bert_dna_weight\\wandb\\run-20230319_175946-mfms6xgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pcoady/Protein-Weightt/runs/mfms6xgk' target=\"_blank\">dazzling-sweep-3</a></strong> to <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pcoady/Protein-Weightt' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/sweeps/fmzpw0ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/mfms6xgk' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/mfms6xgk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zeusg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-3</strong> at: <a href='https://wandb.ai/pcoady/Protein-Weightt/runs/mfms6xgk' target=\"_blank\">https://wandb.ai/pcoady/Protein-Weightt/runs/mfms6xgk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230319_175946-mfms6xgk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mfms6xgk errored: RuntimeError('Cannot find callable resnet110 in hubconf')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mfms6xgk errored: RuntimeError('Cannot find callable resnet110 in hubconf')\n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#run_k_fold(init_config = config) #For k-fold run\n",
    "#wandb.agent(sweep_id, train_loop, count=5) # For sweep Run\n",
    "#train_loop(config) # for single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f22b016a765315302d6d89940da541fcf8d156ba682da1678714b703ab4ccda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
