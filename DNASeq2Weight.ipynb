{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install biopython\n",
    "#!pip install jproperties\n",
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install evaluate\n",
    "#!pip install wandb\n",
    "#!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_51980/3345773149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjproperties\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProperties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#import wandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_scheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequenceClassifierOutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m from .utils import (\n\u001b[0;32m     32\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\transformers\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mto_py_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[1;33m from .hub import (\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mCLOUDFRONT_DISTRIB_PREFIX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mDISABLE_TELEMETRY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfilelock\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileLock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mResponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\huggingface_hub\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mwhoami\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhub_mixin\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelHubMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPyTorchModelHubMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minference_api\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInferenceApi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m from .keras_mixin import (\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\huggingface_hub\\hub_mixin.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\zeusg\\Envs\\real-fast\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import Bio   \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from jproperties import Properties\n",
    "#import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModel, TrainingArguments, Trainer, AutoConfig, DataCollatorWithPadding\n",
    "from transformers import AdamW,get_scheduler\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from motif_utils import seq2kmer # Soruced from https://github.com/jerryji1993/DNABERT\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import json\n",
    "from load_data import create_dataset, explode_dna\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.onnx import FeaturesManager\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import pprint\n",
    "#from google.colab import drive\n",
    "#!pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Normalize dataset\n",
    "2. Try with smaller networks?\n",
    "3. Try with very few training examples to try and get overfitting\n",
    "4. Expand model size\n",
    "5. Hyper Param Search https://huggingface.co/blog/ray-tune\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, val, test = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab=False\n",
    "if colab:\n",
    "    drive.mount('/content/gdrive/', force_remount=True)\n",
    "    %cd gdrive/MyDrive/milestone_data/\n",
    "    with open(\"capstone_body_weight_Statistical_effect_size_analysis_genotype_early_adult_scaled_13022023_gene_symbol_harmonized.pkl\", 'rb') as file:\n",
    "        effect =  pickle.load(file)\n",
    "        \n",
    "        \n",
    "    with open(\"gene_symbol_dna_sequence_exon.pkl\", 'rb') as file:\n",
    "            exon =  pickle.load(file)\n",
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \n",
    "    Arguments:\n",
    "    seq -- str, original sequence.\n",
    "    k -- int, kmer of length k specified.\n",
    "    \n",
    "    Returns:\n",
    "    kmers -- str, kmers separated by space\n",
    "    \"\"\" \n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/gene_symbol_dna_sequence.pkl\", 'rb') as file:\n",
    "        gene =  pickle.load(file)\n",
    "        \n",
    "with open(\"./data/capstone_body_weight_Statistical_effect_size_analysis_genotype_early_adult_scaled_13022023_gene_symbol_harmonized.pkl\", 'rb') as file:\n",
    "        effect =  pickle.load(file)\n",
    "        \n",
    "        \n",
    "with open(\"./data/gene_symbol_dna_sequence_exon.pkl\", 'rb') as file:\n",
    "        exon =  pickle.load(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = exon.Sequence.str.len()))\n",
    "fig.add_trace(go.Histogram(x = gene.Sequence.str.len()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_sequence(seq_list):\n",
    "    return max(seq_list, key =len)\n",
    "\n",
    "def filter_data(effect, exon, min_seq_len = 0, max_seq_len = 512, longest_only = False):\n",
    "    '''\n",
    "    Filter the Data down to include only sequences within a certian size range\n",
    "    Optionally includ only the genes with the longest sequences\n",
    "    '''\n",
    "    effect = effect[['gene_symbol','est_f_ea','p_f_ea']].copy()\n",
    "    \n",
    "    trimmed = exon[(exon.Sequence.str.len()>min_seq_len) & (exon.Sequence.str.len() <= max_seq_len)].copy()\n",
    "    \n",
    "    if longest_only:\n",
    "        trimmed = trimmed.groupby(by=[\"Gene name\"])[\"Sequence\"].apply(list)\n",
    "        trimmed = trimmed.apply(get_longest_sequence)\n",
    "        trimmed = pd.DataFrame({\"Gene name\": trimmed.index, 'Sequence': trimmed.values})\n",
    "\n",
    "    final = pd.merge(effect, trimmed, left_on=\"gene_symbol\", right_on=\"Gene name\")\n",
    "    return final[[\"Gene name\", \"est_f_ea\", \"Sequence\"]]\n",
    "df = filter_data(effect, exon, 75, 512, True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df, size = None):\n",
    "    '''\n",
    "    Preps the data for the ML pipeline\n",
    "    1. Renames the columns to whats required by Hugging face\n",
    "    2. Reduces the number of columns to just what's needed\n",
    "    3. Randomizes the order of the data\n",
    "    4. Splits into Train/val/test sets\n",
    "    5. Scales the data to fit between -1,1 (Which is the range of Tanh)\n",
    "    '''\n",
    "    if size:\n",
    "        df = df.copy().sample(frac=1)[:size]\n",
    "    \n",
    "    \n",
    "    df_len = len(df)\n",
    "    df = df.rename({\"Sequence\":\"dna_seq\", \"est_f_ea\":\"label\"}, axis=1)\n",
    "    df = df[[\"dna_seq\", \"label\"]]\n",
    "    df.dna_seq = df.dna_seq.astype(str)\n",
    "    df.dna_seq = df.dna_seq.apply(lambda x: seq2kmer(x, 6))\n",
    "    df = df.sample(frac=1)\n",
    "    train = df[:int(np.round(df_len*.8))].copy()\n",
    "    val = df[int(np.round(df_len*.8)):int(np.round(df_len*.9))].copy()\n",
    "    test = df[int(np.round(df_len*.9)):].copy()\n",
    "\n",
    "    scaler = MinMaxScaler((-1,1))\n",
    "    scaler.fit(train[\"label\"].values.reshape(-1, 1))\n",
    "    train[\"label\"] = scaler.transform(train[\"label\"].values.reshape(-1, 1))\n",
    "    val[\"label\"] = scaler.transform(val[\"label\"].values.reshape(-1, 1))\n",
    "    test[\"label\"] = scaler.transform(test[\"label\"].values.reshape(-1, 1))\n",
    "    \n",
    "    return train, val, test\n",
    "    \n",
    "train, val, test = data_prep(df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Confirm the distributions of the 3 sets is similar'''\n",
    "def show_dist():\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x = val[\"label\"], histnorm='probability', name = \"val\"))\n",
    "    fig.add_trace(go.Histogram(x = test[\"label\"], histnorm='probability', name = \"test\"))\n",
    "    fig.add_trace(go.Histogram(x = train[\"label\"], histnorm='probability', name = \"train\"))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        barmode=\"overlay\",\n",
    "        bargap=0.1)\n",
    "    fig.show()\n",
    "show_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train.label.mean()\n",
    "val_mean = val.label.mean()\n",
    "test_mean = test.label.mean()\n",
    "\n",
    "print(f\"Train: {test_mean}, Val: {val_mean}, Test: {test_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tokenize the data set, and put them into dataloaders'''\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(df[\"dna_seq\"], padding=True, truncation=True, max_length=512)#512\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(train).map(tokenize_function, batched=True)\n",
    "val = Dataset.from_pandas(val).map(tokenize_function, batched=True)\n",
    "test = Dataset.from_pandas(test).map(tokenize_function, batched=True)\n",
    "\n",
    "train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768,384)\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.fc3 = nn.Linear(192,8)\n",
    "        self.output = nn.Linear(8,1)\n",
    "        self.loss_fct = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        original_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = self.relu(self.dropout(self.fc1(original_output[0])))\n",
    "        outputs = self.relu(self.dropout(self.fc2(outputs[:,0,:])))\n",
    "        outputs = self.relu(self.dropout(self.fc3(outputs)))\n",
    "        logits  = self.tanh(self.output(outputs))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.squeeze(1), labels)\n",
    "        wandb.log({'True Values': labels })\n",
    "        wandb.log({'Predicted Values': logits.squeeze(1)})\n",
    "            \n",
    "        \n",
    "        return SequenceClassifierOutput(loss = loss, logits=logits, hidden_states=original_output.hidden_states, attentions=original_output.attentions)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "   'method': 'grid',\n",
    "   'parameters': {\n",
    "        'learning_rate':{\n",
    "                'values':[1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "        }\n",
    "   }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Hyper Parameters\n",
    "'''\n",
    "num_epochs = 25\n",
    "learning_rate = 5e-5\n",
    "custom_head = True\n",
    "\n",
    "\n",
    "#config = {\n",
    "#        'learning_rate':1e-2,\n",
    "#        \"warmup_steps\":0,\n",
    "#        \"epochs\": num_epochs,\n",
    "#        \"batch_size\": batch_size,\n",
    "#        \"custom_head\": custom_head,\n",
    "#        \"train_size\": len(train),\n",
    "#        \"device\":\"cuda\",\n",
    "#}\n",
    "sweep_config = {\n",
    "   'method': 'grid',\n",
    "   'parameters': {\n",
    "        'learning_rate':{\n",
    "                'values':[1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "        },\n",
    "        \"warmup_steps\":{\n",
    "                'value':0},\n",
    "        \"epochs\": {\n",
    "                'value':num_epochs},\n",
    "        \"batch_size\": {\n",
    "                'value':[4,6,8,10,20]},\n",
    "        \"custom_head\": {\n",
    "                'value':custom_head},\n",
    "        \"train_size\": {\n",
    "                'value':len(train)},\n",
    "        \"device\":{\n",
    "                'value':\"cuda\"},\n",
    "   }\n",
    "}\n",
    "metric = {\n",
    "    'name': 'Validation Epoch Loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DNA-Weight\", entity=\"pcoady\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config = None):\n",
    "    with wandb.init(project=\"DNA-Weight\", entity=\"pcoady\",tags = [\"custom_head\"], config = config):\n",
    "        \n",
    "        config = wandb.config\n",
    "        \n",
    "        train_dataloader = DataLoader(train, shuffle=True, batch_size=config.batch_size, collate_fn=data_collator)\n",
    "        val_dataloader = DataLoader(val, shuffle=True, batch_size=config.batch_size, collate_fn=data_collator)\n",
    "        \n",
    "        model=CustomModel(checkpoint='zhihan1996/DNA_bert_6').to(config.device)\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        num_training_steps = num_epochs * len(train)\n",
    "        loss_fct = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=config.warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "\n",
    "        \n",
    "        progress_bar_train = tqdm(range(num_training_steps))\n",
    "        progress_bar_eval = tqdm(range(num_epochs * len(val)))\n",
    "\n",
    "\n",
    "        dummy_labels = torch.tensor([np.array(train.data.__getitem__(1)).mean() for x in range(train.data.num_rows)])\n",
    "        train_baseline = loss_fct(dummy_labels, torch.tensor(np.array(train.data.__getitem__(1))))\n",
    "        val_baseline = loss_fct(dummy_labels[:val.data.num_rows], torch.tensor(np.array(val.data.__getitem__(1))))\n",
    "        val_step = 0    \n",
    "        epoch_count = 0\n",
    "        wandb.watch(model, log_freq=100)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_train_loss = 0.0\n",
    "            running_val_loss = 0.0\n",
    "\n",
    "            model.train()\n",
    "            train_pred = np.array([])\n",
    "            train_real = np.array([])\n",
    "            for batch in train_dataloader: \n",
    "                batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "                outputs = model(**batch)        \n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                wandb.log({f'Train Baseline':train_baseline, f'Train Batch Loss': outputs.loss})\n",
    "                running_train_loss+=float(outputs.loss)\n",
    "                progress_bar_train.update(1)\n",
    "\n",
    "                #train_pred = np.append(train_pred, outputs.logits.detach().cpu().numpy().flatten())\n",
    "                #train_real = np.append(train_real, batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "            model.eval()\n",
    "            val_pred = np.array([])\n",
    "            val_real = np.array([])\n",
    "            for batch in val_dataloader:\n",
    "                val_step +=1\n",
    "                progress_bar_eval.update(1)\n",
    "                batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**batch)\n",
    "                #val_pred = np.append(val_pred, outputs.logits.detach().cpu().numpy().flatten())\n",
    "                #val_real = np.append(val_real, batch[\"labels\"].cpu().numpy())\n",
    "                wandb.log({f'Validation Baseline':val_baseline, f'Validation Batch Loss':  outputs.loss, \"Val Step\": val_step})\n",
    "                running_val_loss+=float(outputs.loss)\n",
    "\n",
    "            wandb.log({f\"Train Epoch Loss\":running_train_loss/len(train)})\n",
    "            wandb.log({f\"Validation Epoch Loss\":running_val_loss/len(val)})\n",
    "            wandb.log({f'Train R2': r2_score(train_pred, train_real)})\n",
    "            wandb.log({f'Val R2': r2_score(val_pred, val_real), 'Epoch Count': epoch_count})\n",
    "\n",
    "            epoch_count +=1\n",
    "\n",
    "if custom_head:\n",
    "    wandb.agent(sweep_id, train_model, count=5)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_metric = evaluate.load(\"mse\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    #labels = labels.reshape(-1, 1)\n",
    "    baseline = np.array([val_mean for x in range(len(labels))])\n",
    "    new_logits = logits.reshape(1,-1)[0]\n",
    "    \n",
    "    wandb.log({'Base Line': mse_metric.compute(predictions=baseline, references=labels)})\n",
    "    wandb.log({'True Values': labels })\n",
    "    wandb.log({'Predicted Values': new_logits})\n",
    "\n",
    "    mse = mse_metric.compute(predictions=new_logits, references=labels)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_head == False:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('zhihan1996/DNA_bert_6',\n",
    "                                                               num_labels=1, \n",
    "                                                               ignore_mismatched_sizes=True).to(\"cuda\")\n",
    "\n",
    "    training_args = TrainingArguments(output_dir='weight_model', \n",
    "                                      evaluation_strategy='epoch',\n",
    "                                      per_device_train_batch_size = 5,  \n",
    "                                      num_train_epochs=1000)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=val,#CHANGED\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_head == False\n",
    "    results = trainer.predict(train)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=results[1], histnorm='probability', name = \"actual\"))\n",
    "    fig.add_trace(go.Histogram(x=results[0].reshape(-1, 1)[:,0], histnorm='probability', name = \"predictions\"))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        barmode=\"overlay\",\n",
    "        bargap=0.1)\n",
    "    \n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model():\n",
    "        feature = \"sequence-classification\"\n",
    "        model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "        onnx_config = model_onnx_config(model.config)\n",
    "        onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "                preprocessor=tokenizer,\n",
    "                model=model,\n",
    "                config=onnx_config,\n",
    "                opset=13,\n",
    "                output=Path(\"pretrained-model.onnx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f22b016a765315302d6d89940da541fcf8d156ba682da1678714b703ab4ccda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
